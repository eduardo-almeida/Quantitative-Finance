1. Árvores de Decisão
Um modelo ideal para cientistas de dados é a floresta aleatória, e o bloco de construção de uma floresta aleatória é uma árvore de decisão. Vamos aprender como usar árvores de decisão para prever mudanças futuras de preços de títulos.

2. Árvores de decisão
As árvores de decisão possuem nós onde dividem os dados em grupos com base nos recursos. As árvores começam em um nó raiz e terminam com nós folha.

3. Árvores de decisão
As árvores dividem os dados com base em recursos para obter as melhores previsões possíveis. No caso da classificação binária, tentaríamos agrupar todos os 0s de um lado e os 1s do outro. A árvore usa a "pureza" dos nós folha para escolher o melhor recurso para fazer divisões em cada nó. Pureza é a uniformidade dos alvos em um nó folha.

4. Divisões da árvore de decisão
Para dados categóricos, as árvores de decisão se dividem em valores sendo um tipo de categoria. Por exemplo, se o dia da semana for sexta-feira, a árvore poderá dividir os dados com base no dia da semana ser sexta-feira.

5. Divisões da árvore de decisão
Para recursos numéricos, as árvores tentam dividir em cada valor exclusivo do recurso nos dados. Por exemplo, poderíamos dividir na alteração de preço de 10 dias anteriores. Nesse conjunto de dados hipotético, a meta é 1 sempre que a mudança de preço dos 10 dias anteriores for maior que 10%.

6. Árvore ruim
As árvores de regressão usam uma redução na variância, ou dispersão dos dados, para decidir sobre as melhores divisões. Se uma divisão terminar com valores alvo espalhados nas folhas, isso é ruim.

7. Boa árvore
Se uma divisão produzir nós folha com valores de destino fortemente agrupados, esta é uma boa divisão. Podemos medir a propagação dos alvos em uma folha com variância ou desvio padrão.

8. Regressão da árvore de decisão
Depois que nossa árvore de decisão é criada, fazemos previsões calculando a média dos alvos do conjunto de treinamento que terminaram em cada nó folha.

9. Árvores de regressão
Criar e ajustar árvores de decisão é fácil com sklearn. Importamos o DecisionTreeRegressor, criamos o modelo e simplesmente usamos o método fit, fornecendo a ele train_features e train_targets como argumentos. Também temos um grande número de configurações que podemos escolher ao criar o regressor, mas veremos apenas max_depth aqui.

10. Hiperparâmetros da árvore de decisão
max_depth é um hiperparâmetro, que é uma configuração que escolhemos para nosso modelo. Isso controla a altura que nossa árvore pode crescer. Árvores sem limite para max_depth são como esta árvore enorme e se encaixam exatamente no conjunto de treinamento e se saem mal em novos dados; então, definitivamente queremos limitar a profundidade.

11. Profundidade máxima de 3
Árvores com um limite para max_depth podem ser como esta, que tem max_depth de 3.

12. Avalie o modelo
Assim que o modelo estiver ajustado, queremos verificar o desempenho. Veremos primeiro os resultados do treinamento e dos testes. O método de pontuação embutido nos modelos de regressor sklearn calcula a pontuação R-quadrada com base em recursos e alvos. Essencialmente, R-quadrado de 1 significa previsões perfeitas, 0 significa previsões inúteis e valores negativos significam que nossas previsões são horríveis. O valor de 0-ponto-66 no conjunto de trem significa que nos saímos bem, mas o -0-ponto-089 R ao quadrado significa que não estamos indo bem no conjunto de teste. Também espalharemos as previsões em relação aos valores reais para verificar o desempenho. Primeiro, obtemos predições de nosso modelo com predição e recursos, depois dispersamos predições no eixo x e alvos reais no eixo y. Finalmente, mostramos a legenda para sabermos quais pontos são quais, e mostramos o gráfico.

13. Previsões da árvore de decisão
Aqui temos ótimas previsões no conjunto de treinamento e previsões ruins no conjunto de teste. Este é um exemplo de overfitting, que aprenderemos como consertar no capítulo 3.

14. Cultive algumas árvores!
Ok, acho que você está pronto. Vamos plantar algumas árvores!